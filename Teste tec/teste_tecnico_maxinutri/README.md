
# ğŸš€ Desafio TÃ©cnico - Engenharia de Dados | Maxinutri

Este projeto tem como objetivo demonstrar habilidades em ingestÃ£o de dados, modelagem dimensional, ETL e orquestraÃ§Ã£o de pipelines com Apache Airflow, conforme solicitado no desafio tÃ©cnico da Maxinutri.

---

## ğŸ“Œ VisÃ£o Geral

- **Fonte de Dados:** API paginada fornecida pela Maxinutri.
- **Armazenamento:** PostgreSQL.
- **OrquestraÃ§Ã£o:** Apache Airflow.
- **Formato de Modelagem:** Estrela.
- **Script:** Desenvolvido em Python utilizando `pandas`, `sqlalchemy` e `requests`.

---

## âš™ï¸ Arquitetura da SoluÃ§Ã£o

```mermaid
graph TD
    API[API Maxinutri]
    API -->|PaginaÃ§Ã£o + Requests| Ingestao[IngestÃ£o de Dados (Python)]
    Ingestao -->|TransformaÃ§Ã£o| Transformacao[TransformaÃ§Ãµes + Modelagem Dimensional]
    Transformacao -->|Carga| PostgreSQL[(Data Warehouse)]
    PostgreSQL --> Airflow[Airflow DAG]
````

---

## ğŸ§  Justificativas TÃ©cnicas

* **Python**: Linguagem flexÃ­vel e poderosa para manipulaÃ§Ã£o de dados e integraÃ§Ã£o com APIs.
* **PostgreSQL**: Banco relacional robusto, ideal para modelagem dimensional.
* **Airflow**: Usado para orquestrar, monitorar e agendar o pipeline de forma reutilizÃ¡vel e escalÃ¡vel.
* **Modelagem Estrela**: Simplifica consultas e otimiza performance em anÃ¡lises OLAP.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```bash
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_maxinutri_to_postgres.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ README.md
```

---

## ğŸ§± Modelagem Dimensional

### ğŸ”¹ Fatos

**fato\_vendas**

* `order_id` (FK)
* `customer_id` (FK)
* `product_id` (FK)
* `price`
* `frete`
* `data_entrega`

**fato\_avaliacoes**

* `order_id` (FK)
* `nota_avaliacao`
* `titulo_avaliacao`
* `texto_avaliacao`
* `data_avaliacao`
* `data_resposta`

### ğŸ”¸ DimensÃµes

**dim\_cliente**

* `customer_id` (PK)
* `cidade`
* `estado`
* `cep_prefixo`

**dim\_produto**

* `product_id` (PK)
* `categoria`
* `fotos_qtd`
* `peso_g`
* `comprimento_cm`
* `altura_cm`
* `largura_cm`

**dim\_pedido**

* `order_id` (PK)
* `status_pedido`
* `data_compra`
* `data_aprovacao`
* `data_envio_transportadora`
* `data_entrega_cliente`
* `data_entrega_estimada`

---

## ğŸ—ï¸ Setup do Projeto

### PrÃ©-requisitos

* Docker + Docker Compose

### Subindo a stack

```bash
docker-compose up 
```

---

## âœ… Pontos de ValidaÃ§Ã£o

* âœ… Dados coletados da API paginada.
* âœ… Modelagem dimensional aplicada (modelo estrela).
* âœ… ETL implementado e automatizado via Airflow.
* âœ… Dados carregados no PostgreSQL com integridade referencial.
* âœ… CÃ³digo limpo, modular e reutilizÃ¡vel.
* âœ… Logs descritivos e tratamento bÃ¡sico de falhas (timeout, tentativas).

---

## ğŸ’¡ Melhorias Futuras

* Implementar notificaÃ§Ãµes (e-mail/Slack) via Airflow em falhas.
* PersistÃªncia em CSV/Parquet como backup local.
* IndexaÃ§Ã£o e particionamento no PostgreSQL para escalabilidade.
* Painel de monitoramento (ex: Grafana + Prometheus).
* CriaÃ§Ã£o incremental em vez de `replace` nas tabelas.


**Obrigado pela oportunidade!**


